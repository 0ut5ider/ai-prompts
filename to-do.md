- [ ] Add some sort of review agent. maybe at the end of each phase to double check the work of that phase was indeed done correctly, and then decide if the orcjestrator should continue to the next phase. Maybe also have a final reviewer to take a look at the changes that were commited and see if they are indeed in line with the original plan. If not, whay changes, why did it change and record that insight.--- ALREADY EXISTS 



Your answers reveal several structural holes. Here's what I see:
---
1. No Feedback Loop — The Biggest Missing Piece
Your workflow is a straight line: plan → execute → done. There's no mechanism for the system to learn from its own history. This is the gap that explains your "tech debt accumulates silently" answer and why you said "I mostly trust it" for post-execution review.
What's missing: a /retrospective or /post-mortem command.
After execution completes and you've lived with the code for a few days/weeks, you should have a command that:
- Examines what the plan predicted vs. what actually happened in production/usage
- Records classes of errors the execution made (not just individual bugs — patterns)
- Feeds forward into a docs/lessons.md or docs/failure-patterns.md file that /write-plan is instructed to consult
- Tags lessons by category: "testing gap," "architecture assumption violated," "edge case class missed," "integration issue"
Right now your investigation protocol checks ADRs, decision logs, and git history. It never checks "what went wrong last time we did something like this." That's the most valuable signal for planning and you're throwing it away.
---
2. No Circuit Breaker for Mid-Execution Failure
You said there's no mechanism for when a plan is fundamentally wrong mid-execution. The orchestrator just plows through all phases sequentially. This is a real risk, especially because you said you mostly trust the output.
What's missing: phase gates in /execute-plan.
Between phases, the orchestrator should run a lightweight check:
- Do the tests from the just-completed phase actually pass? (Not "did the subagent say they pass" — actually run them.)
- Does the handoff report flag any deviations from the plan? If so, are they within tolerance or do they indicate the plan's assumptions have been invalidated?
- Is the codebase still in a compilable/runnable state?
If any gate fails, the orchestrator should stop, commit what's done, and surface a structured "execution halted" report with the specific assumption that broke. Then you can re-plan from that point rather than letting 3 more phases build on a broken foundation.
This is especially important given your team context — if someone else merges something while your 5-phase execution is running, Phase 4 might be building on a codebase that's already changed.
---
3. No Tech Debt Tracking
You identified this as a real problem. The workflow produces working code but doesn't track what it's degrading. The Augster prompt has PurityAndCleanliness as a maxim, but that's aspirational — there's no mechanism to actually measure it.
What's missing: a /debt-audit command or a tech debt section in execution reports.
Each execution subagent should be required to flag:
- New coupling it introduced (did this phase make two previously independent modules depend on each other?)
- Test coverage it didn't add (code paths that work but aren't tested)
- Hardcoded values, magic numbers, TODO comments it created
- Performance characteristics it didn't measure
- API surface changes that aren't documented
These get appended to a docs/debt-register.md — a running ledger. /write-plan should consult the debt register so that plans can bundle debt paydown with feature work rather than letting it compound indefinitely.
---
4. No Dependency/Impact Analysis Before Planning
Your /write-plan checks ADRs, decision logs, and git history. It doesn't check what depends on the code you're about to change.
What's missing: an impact analysis step in the planning phase.
Before the Devil's Advocate even starts interrogating, the agent should:
- Map the dependency graph of the affected modules (what calls this? what does this call?)
- Identify other in-flight plans that touch the same files (are you about to create a merge conflict with work someone else is doing?)
- Check if the area has high churn (frequently modified = fragile, deserves more caution)
- Identify integration boundaries (does this change cross an API boundary? does it affect other services?)
This matters more on a small team where multiple people might be working on overlapping areas. Your workflow currently plans in isolation — it knows the history of a file but not its current entanglements.
---
5. No Post-Execution Human Review Prompt
You said "I mostly trust it." That's a problem, and your workflow enables it by ending with a verification report that's generated by another AI agent. AI reviewing AI is a weak signal.
What's missing: a structured human review checklist generated per execution.
After /execute-plan finishes, it should produce a docs/reports/{run}/REVIEW.md tailored to what was actually changed:
- "These 3 files had the most complex changes — prioritize reviewing them"
- "These architectural decisions were made during execution (not in the plan) — verify they're acceptable"
- "These test cases cover the happy path but not specific edge cases — consider whether that's sufficient"
- "The plan assumed X; execution confirmed/violated that assumption — verify"
The point isn't to make you review everything. It's to make the minimum effective review obvious so you can spend 10 minutes on the things that matter instead of either reviewing nothing or reviewing everything.
---
6. No Cross-Project Knowledge Transfer
You said the same class of bug hits across projects. Your workflow is entirely project-scoped — docs/ lives inside each project, the investigation protocol only looks at the current project's history.
What's missing: a global knowledge base that travels with you.
A ~/.ai-workflow/patterns.md or similar that records:
- Recurring failure patterns you've seen (e.g., "AI consistently underestimates the complexity of database migration rollback paths")
- Architectural heuristics you've validated across projects
- Model-specific quirks ("MiniMax M2.5 tends to hallucinate test assertions that look plausible but test the wrong invariant")
The Devil's Advocate agent and /write-plan should both consult this. Right now each project starts from zero institutional knowledge.
---
7. No Degradation Detection Between Runs
Related to tech debt, but distinct: you have no mechanism to compare the health of the codebase before and after an execution run.
What's missing: before/after metrics capture.
Before /execute-plan starts, snapshot:
- Test count and pass rate
- Build time
- Binary/bundle size
- Linter warning count
- Dependency count
- Lines of code in touched modules
After execution, snapshot again. Include the delta in the verification report. This makes invisible degradation visible. A single run adding 200 lines and 0 tests is a signal. You won't notice it without measurement.
---
8. No Rollback Protocol
If an execution run produces bad code that gets merged, what's the recovery path? Your workflow has git commit per phase but no documented rollback strategy.
What's missing: rollback instructions per run.
Each execution run should produce a ROLLBACK.md with:
- The exact commits it created (in order)
- Which commits are safe to revert independently vs. which have dependencies
- Any database migrations, config changes, or external state changes that aren't captured in git
---
Summary: Priority Order
| Gap | Impact | Effort |
|-----|--------|--------|
| Feedback loop / retrospective | Highest — you're flying blind on repeat failures | Medium |
| Circuit breaker / phase gates | High — prevents cascading execution on broken assumptions | Low |
| Structured human review prompt | High — "mostly trust it" is a time bomb | Low |
| Tech debt register | Medium-high — you already feel this problem | Medium |
| Cross-project knowledge base | Medium — prevents repeating mistakes across projects | Low |
| Before/after metrics | Medium — makes invisible degradation visible | Low |
| Impact analysis | Medium — matters more as team grows | Medium |
| Rollback protocol | Low-medium — insurance policy | Low |
The feedback loop is the one I'd build first. Everything else is incremental improvement; that one changes the fundamental architecture from open-loop to closed-loop.
Probing question: You said the docs get read "occasionally when something's gone wrong." That means the documentation system is functioning as incident response archaeology, not as planning input. Is that acceptable to you, or do you actually want the docs to influence future planning? Because if you don't close the loop — if /write-plan never consults what went wrong last time — the entire documentation infrastructure is just expensive journaling.